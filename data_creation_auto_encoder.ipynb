{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's create a Synthetic Data using an Auto-encoder in Python\n",
    "Explanation:\n",
    "1) Prepare the Data: Normalize the small real-world dataset.\n",
    "2) Define the Autoencoder Model: Build a simple autoencoder with an encoder and a decoder.\n",
    "3) Train the Autoencoder: Train the autoencoder to learn the compressed representation and reconstruction of the data.\n",
    "4) Generate Synthetic Data: Sample from the latent space of the autoencoder and use the decoder to generate synthetic data.\n",
    "\n",
    "Further Refinements:\n",
    "1. Data Augmentation: Apply small perturbations to the existing data to create more training samples.\n",
    "2. Bootstrapping: Resample the existing data with replacement to create multiple training datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample limited real-world data\n",
    "# Let's create a small dataset for demonstration purposes\n",
    "real_data = np.array([[25, 50000, 1], [30, 60000, 2], [22, 40000, 1], [35, 80000, 3], [28, 55000, 2]])\n",
    "real_data = (real_data - real_data.mean(axis=0)) / real_data.std(axis=0)  # Normalize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder model\n",
    "input_dim = real_data.shape[1]\n",
    "encoding_dim = 2  # Dimensionality of the latent space\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(encoding_dim, activation=\"relu\")(input_layer)\n",
    "decoder = Dense(input_dim, activation=\"sigmoid\")(encoder)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the autoencoder\n",
    "autoencoder.fit(real_data, real_data, epochs=1000, batch_size=2, shuffle=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "encoded_data = np.random.normal(size=(1000, encoding_dim))  # Sample from the latent space\n",
    "synthetic_data = autoencoder.predict(encoded_data)\n",
    "synthetic_data = synthetic_data * real_data.std(axis=0) + real_data.mean(axis=0)  # De-normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "synthetic_data_df = pd.DataFrame(synthetic_data, columns=['Age', 'Income', 'Education Level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the synthetic data\n",
    "print(synthetic_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save to a CSV file\n",
    "synthetic_data_df.to_csv('synthetic_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
